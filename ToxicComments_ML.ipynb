{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING \n",
    "## Topic: Toxic Comment Classification\n",
    "### In this notebook, we use different classifier models to classify the toxic comments.\n",
    "\n",
    "1. Logistic Regression\n",
    "\n",
    "2. Naive Bayes\n",
    "\n",
    "3. Support Vector Machine (SVM)\n",
    "\n",
    "### Then we use Cross Validation Score to try to optimize the model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kaYNMVqj5yxo"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "##### This is an analysis of Wikipedia comments to create models that identify various types of toxic comments. There is a lot of racist content and swear words in the dataset and some of it will pop up in the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 920,
     "output_extras": [
      {
       "item_id": 23
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32492,
     "status": "ok",
     "timestamp": 1522886463906,
     "user": {
      "displayName": "Jay Speidell",
      "photoUrl": "//lh6.googleusercontent.com/-PA283dAAXaQ/AAAAAAAAAAI/AAAAAAAAGng/nfaP1-4Q3qM/s50-c-k-no/photo.jpg",
      "userId": "112011094715710279880"
     },
     "user_tz": 420
    },
    "id": "It7csY1f5yxs",
    "outputId": "a415ec57-e23e-4d58-b3bf-b46882562dbc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from collections import namedtuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "from sklearn.pipeline import make_union\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global random state and k-fold strategy \n",
    "seed = 42\n",
    "k = 5\n",
    "cv = StratifiedKFold(n_splits=k, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "keYiW4Ws5yx2"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "def print_time(start):\n",
    "    time_now = time.time() - start \n",
    "    minutes = int(time_now / 60)\n",
    "    seconds = int(time_now % 60)\n",
    "    if seconds < 10:\n",
    "        print('Elapsed time was %d:0%d.' % (minutes, seconds))\n",
    "    else:\n",
    "        print('Elapsed time was %d:%d.' % (minutes, seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, sparse=0): \n",
    "    \n",
    "    # Comment length\n",
    "    df['length'] = df.comment_text.apply(lambda x: len(x))\n",
    "    \n",
    "\n",
    "    # Capitalization percentage\n",
    "    def pct_caps(s):\n",
    "        return sum([1 for c in s if c.isupper()]) / (sum(([1 for c in s if c.isalpha()])) + 1)\n",
    "    df['caps'] = df.comment_text.apply(lambda x: pct_caps(x))\n",
    "\n",
    "    # Mean Word length \n",
    "    def word_length(s):\n",
    "        s = s.split(' ')\n",
    "        return np.mean([len(w) for w in s if w.isalpha()])\n",
    "    df['word_length'] = df.comment_text.apply(lambda x: word_length(x))\n",
    "\n",
    "    # Average number of exclamation points \n",
    "    df['exclamation'] = df.comment_text.apply(lambda s: len([c for c in s if c == '!']))\n",
    "\n",
    "    # Average number of question marks \n",
    "    df['question'] = df.comment_text.apply(lambda s: len([c for c in s if c == '?']))\n",
    "    \n",
    "    # Normalize\n",
    "    for label in ['length', 'caps', 'word_length', 'question', 'exclamation']:\n",
    "        minimum = df[label].min()\n",
    "        diff = df[label].max() - minimum\n",
    "        df[label] = df[label].apply(lambda x: (x-minimum) / (diff))\n",
    "\n",
    "    # Strip IP Addresses\n",
    "    ip = re.compile('(([2][5][0-5]\\.)|([2][0-4][0-9]\\.)|([0-1]?[0-9]?[0-9]\\.)){3}'\n",
    "                    +'(([2][5][0-5])|([2][0-4][0-9])|([0-1]?[0-9]?[0-9]))')\n",
    "    def strip_ip(s, ip):\n",
    "        try:\n",
    "            found = ip.search(s)\n",
    "            return s.replace(found.group(), ' ')\n",
    "        except:\n",
    "            return s\n",
    "\n",
    "    df.comment_text = df.comment_text.apply(lambda x: strip_ip(x, ip))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_features(comment_text, data, engineered_features):\n",
    "    new_features = sparse.csr_matrix(df[engineered_features].values)\n",
    "    if np.isnan(new_features.data).any():\n",
    "        new_features.data = np.nan_to_num(new_features.data)\n",
    "    return sparse.hstack([comment_text, new_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels:\n",
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "(159571, 6)\n",
      "\n",
      "Training data\n",
      "['comment_text', 'length', 'caps', 'word_length', 'exclamation', 'question']\n",
      "(159571, 6)\n",
      "\n",
      "Submission data\n",
      "['comment_text', 'length', 'caps', 'word_length', 'exclamation', 'question']\n",
      "(153164, 6)\n",
      "['length', 'caps', 'word_length', 'exclamation', 'question']\n"
     ]
    }
   ],
   "source": [
    "# Reset data and create holdout set. \n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "targets = list(df.columns[2:])\n",
    "df_targets = df[targets].copy()\n",
    "\n",
    "df_sub = pd.read_csv('test.csv', dtype={'id': object}, na_filter=False)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_sub.id.copy()\n",
    "\n",
    "# Feature Engineering\n",
    "df = feature_engineering(df)\n",
    "df_sub = feature_engineering(df_sub)\n",
    "\n",
    "print('Training labels:')\n",
    "print(list(df_targets.columns))\n",
    "print(df_targets.shape)\n",
    "\n",
    "print('\\nTraining data')\n",
    "df.drop(list(df_targets.columns), inplace=True, axis=1)\n",
    "df.drop('id', inplace=True, axis=1)\n",
    "print(list(df.columns))\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "print('\\nSubmission data')\n",
    "df_sub.drop('id', inplace=True, axis=1)\n",
    "print(list(df_sub.columns))\n",
    "print(df_sub.shape)\n",
    "\n",
    "toxic_rows = df_targets.sum(axis=1)\n",
    "toxic_rows = (toxic_rows > 0)\n",
    "targets.append('any_label')\n",
    "df_targets['any_label'] = toxic_rows.astype(int)\n",
    "\n",
    "new_features = list(df.columns[1:])\n",
    "print(new_features)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df, holdout, df_targets, holdout_targets = train_test_split(df, df_targets, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['length', 'caps', 'word_length', 'exclamation', 'question']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def multi_cv(model, data, labels, k=5, nb_features=False, shuffle=True):\n",
    "    cv = StratifiedKFold(n_splits=k, random_state=seed, shuffle=True)\n",
    "\n",
    "    def log_count_ratio(x, y):\n",
    "        x = sparse.csr_matrix(x)\n",
    "\n",
    "        p = abs(x[np.where(y==1)].sum(axis=0))\n",
    "        p = p + 1\n",
    "        p = p / np.sum(p)\n",
    "\n",
    "        q = abs(x[np.where(y==0)].sum(axis=0))\n",
    "        q = q + 1\n",
    "        q = q / np.sum(q)\n",
    "\n",
    "        return np.log(p/q)\n",
    "    \n",
    "    scores = []\n",
    "    r_values = []\n",
    "    for label in labels.columns:\n",
    "        if nb_features:\n",
    "            r = log_count_ratio(data, labels[label])\n",
    "            r_values.append(r)\n",
    "            data = data.multiply(r)\n",
    "            if np.isnan(data.data).any():\n",
    "                data.data = np.nan_to_num(data.data)\n",
    "        score = np.mean(cross_val_score(clone(model), data, labels[label], scoring='f1', cv=cv))\n",
    "        print(label + ' f1 score: %.4f' % score)\n",
    "        scores.append(score)\n",
    "    print('Average (excluding any) f1 score: %.4f' % np.mean(scores[:-1]))\n",
    "    if nb_features:\n",
    "        return scores, r_values\n",
    "    else:\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text must be vectorized before being fed into machine learning models. This is a method of converting textual data into numerical data that a computer can comprehend. The characteristics of vectorized data are generally word counts or another manner of describing the occurance of letters or words in a string, and the data is usually sparse. This is accomplished through the use of a vectorizer object, which holds a dictionary of letters or words, as well as their associated integer representations and pertinent statistics, if appropriate.\n",
    "\n",
    "Term frequency - inverted document frequency is the method we'll employ here. This is a statistic that compares the frequency of a string of characters in a single document (here, a single remark) against the inverse of its frequency over all documents in the dataset to determine its usefulness. That a term that appears often in a comment in this dataset but appears in only a few comments in the dataset is likely valuable to the model. However, a string that appears in practically every page is nearly worthless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TGoGjh6b5y0i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 0:19.\n",
      "(127656, 10000)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "comment_vector = TfidfVectorizer(max_features=10000, analyzer='word', stop_words='english')\n",
    "training_comments = comment_vector.fit_transform(df.comment_text)\n",
    "holdout_comments = comment_vector.transform(holdout.comment_text)\n",
    "submission_comments = comment_vector.transform(df_sub.comment_text)\n",
    "print_time(start)\n",
    "\n",
    "print(training_comments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic score: 0.7203\n",
      "severe_toxic score: 0.3203\n",
      "obscene score: 0.7464\n",
      "threat score: 0.1982\n",
      "insult score: 0.6261\n",
      "identity_hate score: 0.2785\n",
      "any_label score: 0.7295\n",
      "Elapsed time was 0:51.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for target in targets: \n",
    "    lr = LogisticRegression(random_state=seed, max_iter=500)\n",
    "    print(target + ' score: %.4f' % np.mean(cross_val_score(lr, training_comments, df_targets[target], scoring='f1', cv=cv)))\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With engineered features added in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic score: 0.7249\n",
      "severe_toxic score: 0.3485\n",
      "obscene score: 0.7445\n",
      "threat score: 0.2047\n",
      "insult score: 0.6254\n",
      "identity_hate score: 0.2846\n",
      "any_label score: 0.7322\n",
      "Elapsed time was 1:17.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for target in targets: \n",
    "    lr = LogisticRegression(random_state=seed, max_iter=500)\n",
    "    print(target + ' score: %.4f' % np.mean(cross_val_score(lr, merge_features(training_comments, df, new_features), df_targets[target], scoring='f1', cv=cv)))\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic f1 score: 0.6588\n",
      "severe_toxic f1 score: 0.0992\n",
      "obscene f1 score: 0.6637\n",
      "threat f1 score: 0.0000\n",
      "insult f1 score: 0.5625\n",
      "identity_hate f1 score: 0.0451\n",
      "any_label f1 score: 0.6681\n",
      "Average (excluding any) f1 score: 0.3382\n",
      "Elapsed time was 0:03.\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "\n",
    "model = MultinomialNB(alpha=1.0)\n",
    "_ = multi_cv(model, training_comments, df_targets)\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With engineered features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic f1 score: 0.6676\n",
      "severe_toxic f1 score: 0.0966\n",
      "obscene f1 score: 0.6709\n",
      "threat f1 score: 0.0000\n",
      "insult f1 score: 0.5717\n",
      "identity_hate f1 score: 0.0387\n",
      "any_label f1 score: 0.6748\n",
      "Average (excluding any) f1 score: 0.3409\n",
      "Elapsed time was 0:08.\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "\n",
    "model = MultinomialNB(alpha=1.0)\n",
    "_ = multi_cv(model, merge_features(training_comments, df, new_features), df_targets)\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic f1 score: 0.7543\n",
      "severe_toxic f1 score: 0.3386\n",
      "obscene f1 score: 0.7845\n",
      "threat f1 score: 0.3520\n",
      "insult f1 score: 0.6636\n",
      "identity_hate f1 score: 0.3635\n",
      "any_label f1 score: 0.7693\n",
      "Average (excluding any) f1 score: 0.5427\n",
      "Elapsed time was 0:53.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model = LinearSVC(random_state=seed)\n",
    "_ = multi_cv(model, training_comments, df_targets)\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With engineered features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic f1 score: 0.7607\n",
      "severe_toxic f1 score: 0.3600\n",
      "obscene f1 score: 0.7848\n",
      "threat f1 score: 0.3538\n",
      "insult f1 score: 0.6639\n",
      "identity_hate f1 score: 0.3675\n",
      "any_label f1 score: 0.7735\n",
      "Average (excluding any) f1 score: 0.5485\n",
      "Elapsed time was 0:58.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model = LinearSVC(random_state=seed)\n",
    "_ = multi_cv(model, merge_features(training_comments, df, new_features), df_targets)\n",
    "print_time(start)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "kaYNMVqj5yxo",
    "yH8NDq2D5yx9",
    "jxMujLal5yyN",
    "KmQxwcqp5yyu",
    "_A0tzkGe5yyz",
    "41hcbO6m5yy8",
    "qZU1wqtf5yy9",
    "WPNxOBNp5yzJ",
    "QeJWnVa15yzZ",
    "ZSCnkLiC5y0A",
    "krzeQWNb5y0h",
    "X7Yxshbs5y02",
    "1O2erv3G5y07",
    "lw2pO8mo5y0-",
    "aCDEfw7h5y1F",
    "VJj2r6ky5y1O",
    "5yiO9G4V5y1U",
    "c_NzviMo5y1t",
    "fVAFqZCz5y2P",
    "6dEs1c0H5y2r"
   ],
   "default_view": {},
   "name": "ToxicComments.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
